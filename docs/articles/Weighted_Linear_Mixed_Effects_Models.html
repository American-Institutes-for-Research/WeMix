<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Weighted Linear Mixed-Effects Models • WeMix</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Weighted Linear Mixed-Effects Models">
<meta property="og:description" content="WeMix">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">WeMix</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">4.0.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Introduction_to_Mixed_Effects_Models_With_WeMix.html">Introduction to Weighted Mixed-Effects Models With WeMix</a>
    </li>
    <li>
      <a href="../articles/Weighted_Linear_Mixed_Effects_Models.html">Weighted Linear Mixed-Effects Models</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/American-Institutes-for-Research/WeMix/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Weighted Linear Mixed-Effects Models</h1>
                        <h4 data-toc-skip class="author">Developed by
Paul Bailey </h4>
            
            <h4 data-toc-skip class="date">February 14, 2018</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/American-Institutes-for-Research/WeMix/blob/HEAD/vignettes/Weighted_Linear_Mixed_Effects_Models.Rmd" class="external-link"><code>vignettes/Weighted_Linear_Mixed_Effects_Models.Rmd</code></a></small>
      <div class="hidden name"><code>Weighted_Linear_Mixed_Effects_Models.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The <code>WeMix</code> package aims to fit a linear mixed model where
units are nested inside groups,which may themselves be nested in
groups,a model specified by Rabe-Hesketh and Skrondal (2006) and
Rabe-Hesketh, Skrondal, and Pickles (2002) for the GLLAMM software. The
advantage of fitting the model in nested levels is that survey sampling
weights can be incorporated; the pseudo-likelihood, at the top level, is
an integral that sums across the top-level groups, weighting each group
or unit (in a two-level model) according to sampling weights.</p>
<p>At the highest level, the likelihood (<span class="math inline">\(\mathcal{L}\)</span>) is a function of the
parameters for the random-effect covariance matrix (<span class="math inline">\(\bm{\theta}\)</span>), the fixed-effect estimates
(<span class="math inline">\(\bm{\beta}\)</span>), and the outcomes
<span class="math inline">\(\bm{y}\)</span>. This equation relates the
overall likelihood [<span class="math inline">\(\mathcal{L}(\bm{\theta},
\bm{\beta}; \bm{y})\)</span>] to the integral of the likelihoods of the
integrals of the top-level units [<span class="math inline">\(\mathcal{L}^{(L)}_{j}(\bm{\theta}, \bm{\beta} ;
\bm{y}| \bm{u}^{(L)})\)</span>, indexed by <span class="math inline">\(j\)</span>]; the top-level unit likelihoods have a
superscript <span class="math inline">\((L)\)</span> to indicate they
are for the <span class="math inline">\(L\)</span>th (top) level. Here
we omit reference to the fixed-effect design matrix <span class="math inline">\(\bm{X}\)</span> and the random-effect design
matrix <span class="math inline">\(\bm{Z}\)</span>, but the likelihood
is conditional on those terms as well. <span class="math display">\[\begin{align}
\mathcal{L}(\bm{\theta}, \bm{\beta}; \bm{y}) &amp;= \int
g^{(L)}(\bm{u}^{(L)};\bm{\theta}^{(L)})\prod_{j}{
\bigg[\mathcal{L}^{(L)}_{j}(\bm{\theta}, \bm{\beta} ; \bm{y}|
\bm{u}^{(L)})\bigg]^{w^{(L)}_{j}}} d\bm{u}^{(L)}
\end{align}\]</span> where the <span class="math inline">\(\bm{u}\)</span> terms are the random effects,
which are marginalized by integrating over them; the <span class="math inline">\(g\)</span> function plays a role similar to a
prior, but has its variance (or covariance matrix) fit using covariance
parameter vector <span class="math inline">\(\bm{\theta}\)</span>, while
<span class="math inline">\(j\)</span> represents the indexes of all the
top-level units, which have their likelihood raised to the power of
their weight <span class="math inline">\(w^{(L)}_{j}\)</span>. Because
<span class="math inline">\(\bm{u}\)</span> may be a vector—for example,
if there is a random slope and intercept—the covariance matrix between
the <span class="math inline">\(\bm{u}\)</span> terms (<span class="math inline">\(\bm{\Sigma}^{(L)}\)</span>) may be diagonal (no
covariance) or dense. In any case, the covariance matrix is
parameterized by a vector of values <span class="math inline">\(\bm{\theta}\)</span>; at the <span class="math inline">\(L\)</span>th level, the relevant elements are
denoted by <span class="math inline">\(\bm{\theta}^{(L)}\)</span>.</p>
<p>The conditional likelihood at each level from <span class="math inline">\(L\)</span> to two—those above the first, or lowest
level—is then recursively defined, for the <span class="math inline">\(j\)</span>th unit, at level <span class="math inline">\(l\)</span> (<span class="math inline">\(\mathcal{L}^{(l)}_j\)</span>; Rabe-Hesketh et al.,
2002, eq. 3) as: <span class="math display">\[\begin{align}
\mathcal{L}^{(l)}_j(\bm{\theta}, \bm{\beta}; \bm{y}| \bm{U}^{(l+1)}) =
\int g^{(l)}(\bm{u}^{(l)};\bm{\theta}^{(l)})\prod_{j'}{
\bigg[\mathcal{L}^{(l-1)}_{j'}(\bm{\theta}, \bm{\beta} ; \bm{y}|
\bm{U}^{(l)})\bigg]^{\bm{w}^{(l-1)}_{j'}}} d\bm{u}^{(l)} \quad l=2,
\ldots, L-1
\end{align}\]</span> where the subscript <span class="math inline">\(j'\)</span> that the product is indexed over
indicates that the likelihood <span class="math inline">\(\mathcal{L}^{(l-1)}_{j'}(\cdot)\)</span> is
for the units of level <span class="math inline">\(l-1\)</span> nested
in unit <span class="math inline">\(j\)</span>, and <span class="math inline">\(\bm{U}^{(l)}\)</span> is all of the random effects
at or above level <span class="math inline">\(l\)</span>, so that <span class="math inline">\(\bm{U}^{(l)}\)</span> includes <span class="math inline">\(\left\{\bm{u}^{(l)}, \bm{u}^{(l+1)}, \dots,
\bm{u}^{(L)}\right\}\)</span>. This equation reflects the nested nature
of the data; a group (e.g., school), annotated as <span class="math inline">\(j\)</span>, has an individual likelihood that is
the product of the likelihoods of the units or groups (e.g., students or
classrooms, annotated as <span class="math inline">\(j'\)</span>)
nested inside of it.</p>
<p>In the case of a Gaussian residual, the likelihood (<span class="math inline">\(\mathcal{L}^{(1)}\)</span>) at the individual unit
level is given by the equation <span class="math display">\[\begin{align}
\mathcal{L}_i^{(1)}(\bm{\theta}, \bm{\beta};\bm{y},\bm{U}^{(2)}) &amp;=
\frac{1}{\sigma} \phi \left(  \frac{\hat{e}_i^{(1)} }{\sigma}   \right)
\end{align}\]</span> where the subscript <span class="math inline">\(i\)</span> is used to indicate that this is the
likelihood of the <span class="math inline">\(i^{th}\)</span>
individual, the superscript <span class="math inline">\((1)\)</span> on
<span class="math inline">\(\hat{\bm{e}}_i^{(1)}\)</span> is used to
indicate that it is an unpenalized residual—where the penalized residual
will be introduced in the next section—<span class="math inline">\(\phi(\cdot)\)</span> is the standard normal
density function and <span class="math inline">\(\sigma\)</span> is the
residual variance (a scalar), and the residuals vector <span class="math inline">\(\hat{\bm{e}}^{(1)}\)</span> represents the
residuals <span class="math inline">\(\hat{\bm{e}}_i^{(1)}\)</span> for
each individual and is given, in vector form, by <span class="math display">\[\begin{align}
\hat {\bm{e}}^{(1)} &amp;= \bm{y} - \bm{X}\hat{\bm{\beta}} -
\sum_{l=2}^L \bm{Z}^{(l)}\hat{\bm{u}}^{(l)}  \
\end{align}\]</span>\end{align} When solved with the above integrals (as
in Rabe-Hesketh et al., 2002), <span class="math inline">\(\sigma\)</span> is fit as a parameter and there is
no direct equation for it.</p>
<p>This document describes how <code>WeMix</code> uses symbolic
integration that relies on a mix of both Bates and Pinheiro (1998) and
the <code>lme4</code> package (Bates, Maechler, Bolker, and Walker,
2015), obviating the need for numerical integration in a weighted,
nested model.</p>
<p>The basic model in <code>lme4</code> is of the form (Bates et al.,
2015, eqs. 2 and 3): <span class="math display">\[\begin{align}
\left( \bm{y}|\bm{U}=\bm{u}\right) &amp;\sim N(\bm{X\beta} + \bm{Zu},
\sigma^2 W^{-1} ) \label{eq:lme4A}\\
\bm{U} &amp;\sim N(0, \bm{\Sigma}(\bm{\theta})) \label{eq:lme4B}
\end{align}\]</span> where <span class="math inline">\(N(\cdot,
\cdot)\)</span> is the multivariate normal distribution, and <span class="math inline">\(\bm{\Sigma}(\bm{\theta})\)</span> is positive
semidefinite—allowing, for example, a variance parameter to be exactly
zero—that is parameterized by a vector of parameters <span class="math inline">\(\bm{\theta}\)</span>.</p>
<p>The likelihood is maximized by integrating (symbolically) over <span class="math inline">\(\bm{U}\)</span> (Bates et al., 2015, eqs. 20, 21,
22, and 24): <span class="math display">\[\begin{align}
f_{\bm{y}}(\bm{y};\bm{\beta},\bm{\theta}) &amp;= \int
f_{\bm{y}|\bm{U=u}}\left(\bm{y}; \bm{\beta}, \bm{\theta}, \bm{u} \right)
\cdot f_{\bm{U}}(\bm{u}) d\bm{u} \label{eq:lme4C}
\end{align}\]</span></p>
<p>where the <span class="math inline">\(f_{\bm{U}}\)</span> term is
analogous to <span class="math inline">\(g(\bm{u}^{(l)})\)</span> in the
Rabe-Hesketh et al. (2006) formulation but is intentionally represented
in a non-nested structure to allow for crossed terms.</p>
<p>In this document we show how <code>WeMix</code> uses a derivation
similar to that in Bates et al. (2015) and Bates and Pinheiro (1998) to
fit a sample-weighted mixed model, avoiding the integration necessary in
GLLAMM. Comparing <code>WeMix</code> to <code>lmer</code>, the latter is
more general in the sense of allowing crossed terms, while
<code>WeMix</code> allows for sampling weights; unweighted, the models
should agree.</p>
<p>Generally, the Rabe-Hesketh et al. (2006) model can be rewritten in a
form very similar to <code>lme4</code> as <span class="math display">\[\begin{align}
\left( \bm{y}|\bm{U}=\bm{u}\right) &amp;\sim  T_1^{\omega^{(1)}}
\label{eq:WeMixA}\\
\bm{U} &amp;\sim T_2^{\omega^{(2)}} * T_3^{\omega^{(3)}} * \cdots *
T_L^{\omega^{(L)}} \label{eq:WeMixB}
\end{align}\]</span> where <span class="math inline">\(T^k\)</span>
represents the convolution of <span class="math inline">\(k\)</span>
instances of <span class="math inline">\(T\)</span> and <span class="math inline">\(*\)</span> represents the convolution of two
distributions, with a likelihood that is their product; <span class="math inline">\(w^{(l)}\)</span> are the weights assigned to units
(<span class="math inline">\(l=1\)</span>) or groups (<span class="math inline">\(l&gt;1\)</span>) that are ideally the inverse
(unconditional) probability of selecting the unit or group; and the
<span class="math inline">\(T\)</span>s have distribution <span class="math display">\[\begin{align}
T_1 &amp;\sim N(\bm{X\beta} + \bm{Zu}, \sigma^2 I ) \label{eq:WeMixA2}
\\
T_l &amp;\sim N(0, \bm{\Sigma_{ll}}) \quad l=2, \cdots, L
\label{eq:WeMixB2}
\end{align}\]</span> where <span class="math inline">\(\bm{\Sigma}\)</span> is block diagonal,
disallowing nonzero prior correlations across levels, with the <span class="math inline">\(l\)</span>th block being written <span class="math inline">\(\bm{\Sigma}_{ll}\)</span>.</p>
<p>The next section follows Bates et al. (2015) and shows the
<code>lme4</code> and <code>WeMix</code> model without weights—the only
case where they are identical. The subsequent section then shows the
adaptations to the likelihood for the sample weighted case. Notably,
<code>lme4</code> has unit-level weights, but they are precision weights
and not level-1 sample weights, so even when only the level-1 weights
are nontrivial, the models disagree. The final section describes the
application of the profile likelihood (again, following
<code>lme4</code>) used to estimate linear models in
<code>WeMix</code>.</p>
<p>Following the logic of Bates et al. (2015), the unweighted (unit
weight) case simplifies eqs. <span class="math inline">\(\ref{eq:WeMixA}\)</span> and <span class="math inline">\(\ref{eq:WeMixB}\)</span> to <span class="math display">\[\begin{align}
\left( \bm{y}|\bm{U}=\bm{u}\right) &amp;\sim  T_1 \label{eq:WeMixAnoW}\\
\bm{U} &amp;\sim T_2 * T_3 * \cdots * T_L \label{eq:WeMixBnoW}
\end{align}\]</span> where eqs. <span class="math inline">\(\ref{eq:WeMixA2}\)</span> and <span class="math inline">\(\ref{eq:WeMixB2}\)</span> are simplified to <span class="math display">\[\begin{align}
T_1 &amp;\sim N(\bm{X\beta} + \bm{Zu}, \sigma^2 I )
\label{eq:WeMixA2noW} \\
T_l &amp;\sim N(0, \bm{\Sigma_{ll}}) \quad l=2, \cdots, L
\label{eq:WeMixB2noW}
\end{align}\]</span> the random-effect vector <span class="math inline">\(\bm{U}\)</span> is rewritten as the product of a
square root-covariance matrix <span class="math inline">\(\bm{\Lambda}\)</span> and an <span class="math inline">\(iid\)</span> normal vector <span class="math inline">\(\bm{\upsilon}\)</span>: <span class="math display">\[\begin{align}
\bm{U}&amp;=\bm{\Lambda} \bm{\upsilon} \label{eq:Uf} \\
\bm{\upsilon} &amp;\sim N(0, \sigma^2 \bm{I})
\end{align}\]</span> When <span class="math inline">\(\bm{\Lambda}\)</span> is a <em>square root
matrix</em> of <span class="math inline">\(\bm{\Sigma}\)</span>, meaning
<span class="math display">\[\begin{align}
\frac{1}{\sigma^2}\bm{\Sigma}&amp;= \frac{1}{\sigma^2} \bm{\Lambda}^T
\bm{\Lambda} \label{eq:root}
\end{align}\]</span> it follows that <span class="math inline">\(\bm{U}\)</span> has the distribution shown in eq.
<span class="math inline">\(\ref{eq:WeMixB2noW}\)</span>, but the
equations can use the much easier to work with <span class="math inline">\(\bm{\upsilon}\)</span>. Note that eq. <span class="math inline">\(\ref{eq:root}\)</span> implies <span class="math display">\[\begin{align}
\bm{\Sigma} &amp;= \left[ \begin{matrix} \bm{\Sigma}_{11}
&amp;0&amp;\cdots&amp;0 \\ 0 &amp;\bm{\Sigma}_{22}&amp;0&amp;\vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp;0
&amp; \bm{\Sigma}_{LL}\\ \end{matrix}\right] \\
&amp;=\left[ \begin{matrix} \bm{\Lambda}_{11}^T &amp;0&amp;\cdots&amp;0
\\ 0 &amp;\bm{\Lambda}_{22}^T&amp;0&amp;\vdots \\ \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp;0 &amp;
\bm{\Lambda}_{LL}^T\\ \end{matrix}\right]
\left[ \begin{matrix} \bm{\Lambda}_{11} &amp;0&amp;\cdots&amp;0 \\ 0
&amp;\bm{\Lambda}_{22}&amp;0&amp;\vdots \\ \vdots &amp; \vdots &amp;
\ddots &amp; \vdots \\ 0 &amp; \cdots &amp;0 &amp; \bm{\Lambda}_{LL}\\
\end{matrix}\right] \\
&amp;=  \bm{\Lambda}^T \bm{\Lambda}
\end{align}\]</span> Note that <span class="math inline">\(\bm{\Lambda}\)</span> (and thus <span class="math inline">\(\bm{\Sigma}\)</span>) will be parameterized by a
set of parameters <span class="math inline">\(\bm{\theta}\)</span>, so
eq. <span class="math inline">\(\ref{eq:root}\)</span> could be written
<span class="math display">\[\begin{align}
\frac{1}{\sigma^2}\bm{\Sigma}(\bm{\theta})&amp;=
\frac{1}{\sigma^2}\left(\bm{\Lambda}(\bm{\theta})\right)^T
\bm{\Lambda}(\bm{\theta})
\end{align}\]</span> and will be written this way to keep track of the
parameters involved. These parameters vary by model. For a two-level
model with a random intercept and nothing else, <span class="math inline">\(\bm{\theta}\)</span> would be a scalar with the
relative precision of the random effect. The matrix <span class="math inline">\(\bm{\Lambda}({\bm{\theta}})\)</span> would then be
diagonal and of the form <span class="math inline">\(\bm{\Lambda}({\bm{\theta}})=\theta
\bm{I}\)</span>, with <span class="math inline">\(\bm{I}\)</span> being
an identity matrix with the same number of rows and columns as there are
groups (random effects). For a two-level model with a random slope and
intercept, the <span class="math inline">\(\bm{\theta}\)</span> would
have length three and would parameterize the three elements of a
covariance matrix. In this special case, the parameterization could be
<span class="math inline">\(\bm{\Lambda}({\bm{\theta}})=
\left[\begin{matrix} \theta_1 \bm{I} &amp; \theta_2 \bm{I} \\ 0 &amp;
\theta_3 \bm{I} \end{matrix}\right]\)</span>; a block matrix that allows
the slope and intercept term to covary with each other, within a
group.</p>
<p>Then, the residual (penalized) sum of squares is (Bates et al., 2015,
eqs. 14 and 15) <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \lvert\lvert
\bm{y}-\bm{X\beta} - \bm{Z\Lambda(\theta)\upsilon}\rvert\rvert^2 +
||\bm{\upsilon}||^2
\end{align}\]</span> where <span class="math inline">\(||\bm{v}||^2\)</span> is the sum of squares for
the vector <span class="math inline">\(\bm{v}\)</span>; as an equation,
<span class="math inline">\(||\bm{v}||^2=\sum v_i^2\)</span>.</p>
<p>Notice that the penalty function (<span class="math inline">\(||\bm{\upsilon}||^2\)</span>) and the residual
both are assumed to be independent identically distributed normal
distributions with variance <span class="math inline">\(\sigma^2\)</span>; this allows for both the
regression and the random effects to be estimated in one regression
where the pseudo-data outcome for the random effects is a vector of
zeros (Bates et al., 2015, eq. 16). This rewrites <span class="math inline">\(r\)</span> as a sum of squares, adding a vector of
zeros below <span class="math inline">\(\bm{y}\)</span>—the
<em>pseudo-data</em>: <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left| \left[
\begin{matrix} \bm{y} \\ \bm{0} \end{matrix} \right] -\left[
\begin{matrix} \bm{Z\Lambda}(\bm{\theta}) &amp; \bm{X} \\ \bm{I} &amp;
\bm{0} \end{matrix} \right] \left[ \begin{matrix} \bm{\upsilon} \\
\bm{\beta} \end{matrix} \right] \right|\right|^2 \label{eq:WeMixR0}
\end{align}\]</span> Unlike Bates et al. (2015, eq. 16), we proceed by
taking a QR decomposition (Trefethen and Bau, 1997) of <span class="math display">\[\begin{align}
\bm{A} &amp;\equiv \left[ \begin{matrix} \bm{Z\Lambda}(\bm{\theta})
&amp; \bm{X} \\ \bm{I} &amp; \bm{0} \end{matrix} \right] \label{eq:uwA}
\end{align}\]</span> Plugging eq. <span class="math inline">\(\ref{eq:uwA}\)</span> into eq. <span class="math inline">\(\ref{eq:WeMixR0}\)</span> and finding the least
squares solution (denoted with a hat: <span class="math inline">\(\bm{\hat{u}}\)</span> and <span class="math inline">\(\bm{\hat{\beta}}\)</span>) <span class="math display">\[\begin{align}
\bm{A} \left[ \begin{matrix} \hat{\bm{\upsilon}} \\ \hat{\bm{\beta}}
\end{matrix} \right]&amp;=\left[ \begin{matrix} \bm{y} \\ \bm{0}
\end{matrix} \right]
\end{align}\]</span> using the QR decomposition on <span class="math inline">\(\bm{A}\)</span>, which rewrites <span class="math inline">\(\bm{A}=\bm{QR}\)</span> for an orthogonal matrix
<span class="math inline">\(\bm{Q}\)</span> (So, <span class="math inline">\(\bm{Q}^T\bm{Q}=\bm{I}\)</span>) and an upper
triangular matrix <span class="math inline">\(\bm{R}\)</span>, <span class="math display">\[\begin{align}
\bm{QR} \left[ \begin{matrix} \hat{\bm{\upsilon}} \\ \hat{\bm{\beta}}
\end{matrix} \right]&amp;=\left[ \begin{matrix} \bm{y} \\ \bm{0}
\end{matrix} \right]
\end{align}\]</span> where <span class="math inline">\(\bm{R}\)</span>
can be written in block form as <span class="math display">\[\begin{align}
\bm{R} &amp;= \left[ \begin{matrix} \bm{R}_{11} &amp; \bm{R}_{12}
\\  \bm{0} &amp; \bm{R}_{22} \end{matrix} \right] \label{eq:Rblock}
\end{align}\]</span> in which <span class="math inline">\(\bm{R}_{11}\)</span> is also upper triangular,
square, and conformable with <span class="math inline">\(\bm{\upsilon}\)</span>, and <span class="math inline">\(\bm{R}_{22}\)</span> is similarly upper
triangular, square, and conformable with <span class="math inline">\(\bm{\beta}\)</span>, while <span class="math inline">\(\bm{R}_{12}\)</span> is rectangular and
(potentially) dense.</p>
<p>Rewriting <span class="math inline">\(r^2\)</span> as a deviation
from the least squares solution, eq. <span class="math inline">\(\ref{eq:WeMixR0}\)</span> becomes <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left| \left[
\begin{matrix} \bm{y} \\ \bm{0} \end{matrix} \right] - \bm{A} \left[
\begin{matrix} \bm{\upsilon} \\ \bm{\beta} \end{matrix} \right]
\right|\right|^2 \\
&amp;= \left| \left|  \left[ \begin{matrix} \bm{y} \\ \bm{0}
\end{matrix} \right] - \bm{A} \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} + \hat{\bm{\upsilon}} \\ \bm{\beta} -
\hat{\bm{\beta}} + \hat{\bm{\beta}} \end{matrix} \right]
\right|\right|^2 \\
&amp;= \left| \left|  \left[ \begin{matrix} \bm{y} \\ \bm{0}
\end{matrix} \right] - \bm{A} \left[ \begin{matrix}\hat{\bm{\upsilon}}
\\ \hat{\bm{\beta}} \end{matrix} \right] - \bm{A} \left[ \begin{matrix}
\bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}}
\end{matrix} \right] \right|\right|^2
\end{align}\]</span> Using the identity that for any vector <span class="math inline">\(\bm{v}\)</span> the sum of squares is also just
the inner product, so <span class="math inline">\(||\bm{v}||^2 =
\bm{v}^T\bm{v}\)</span>,<br><span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon})&amp;= \left[  \left[
\begin{matrix} \bm{y} \\ \bm{0} \end{matrix} \right] - \bm{A} \left[
\begin{matrix}\hat{\bm{\upsilon}} \\ \hat{\bm{\beta}} \end{matrix}
\right] - \bm{A} \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix}
\right] \right]^T    \left[ \left[ \begin{matrix} \bm{y} \\ \bm{0}
\end{matrix} \right] - \bm{A} \left[ \begin{matrix}\hat{\bm{\upsilon}}
\\ \hat{\bm{\beta}} \end{matrix} \right] - \bm{A} \left[ \begin{matrix}
\bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}}
\end{matrix} \right] \right] \label{eq:finalT}
\end{align}\]</span> Defining <span class="math inline">\(\hat{\bm{e}}\)</span> to be the penalized least
squares residual, <span class="math display">\[\begin{align}
\hat{\bm{e}} \equiv \left[ \begin{matrix} \bm{y} \\ \bm{0} \end{matrix}
\right] - \bm{A} \left[ \begin{matrix}\hat{\bm{\upsilon}} \\
\hat{\bm{\beta}} \end{matrix} \right]
\end{align}\]</span> then eq. <span class="math inline">\(\ref{eq:finalT}\)</span> can be rewritten <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left[  \hat{\bm{e}}
- \bm{A} \left[ \begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\
\bm{\beta} - \hat{\bm{\beta}} \end{matrix} \right] \right]^T    \left[
\hat{\bm{e}} - \bm{A} \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix}
\right] \right] \\
&amp;= \hat{\bm{e}}^T \hat{\bm{e}} - 2 \hat{\bm{e}}^T    \left[ \bm{A}
\left[ \begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta}
- \hat{\bm{\beta}} \end{matrix} \right] \right] + \left[  \bm{A} \left[
\begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} -
\hat{\bm{\beta}} \end{matrix} \right] \right]^T    \left[ \bm{A} \left[
\begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} -
\hat{\bm{\beta}} \end{matrix} \right] \right] \label{eq:quad}
\end{align}\]</span> Since <span class="math inline">\(\hat{\bm{e}}\)</span>, the uniquely minimized
residuals to the least squares problem is in the null of <span class="math inline">\(\bm{A}\)</span>, while <span class="math inline">\(\bm{Ax}\)</span> is in the span of <span class="math inline">\(\bm{A}\)</span> for any vector <span class="math inline">\(\bm{x}\)</span>, then <span class="math inline">\(\hat{\bm{e}}^T \bm{Ax}=0\)</span> for any <span class="math inline">\(\bm{x}\)</span>. Thus, <span class="math inline">\(\hat{\bm{e}}^T \left[ \bm{A} \left[ \begin{matrix}
\bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}}
\end{matrix} \right] \right]=\bm{0}\)</span> and eq. <span class="math inline">\(\ref{eq:quad}\)</span> becomes <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \hat{\bm{e}}^T
\hat{\bm{e}} + \left[ \begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}}
\\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix} \right]^T \bm{A}^T \bm{A}
\left[ \begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta}
- \hat{\bm{\beta}} \end{matrix} \right] \\
&amp;= \hat{\bm{e}}^T \hat{\bm{e}}   + \left[ \begin{matrix}
\bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}}
\end{matrix} \right]^T \left[ \bm{QR} \right]^T
\left[\bm{QR}\right]    \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix}
\right] \\
&amp;= \hat{\bm{e}}^T \hat{\bm{e}}  + \left[ \begin{matrix}
\bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}}
\end{matrix} \right]^T \bm{R}^T \bm{Q}^T \bm{Q} \bm{R} \left[
\begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} -
\hat{\bm{\beta}} \end{matrix} \right]
\end{align}\]</span> Then, because <span class="math inline">\(\bm{Q}\)</span> is orthonormal, <span class="math inline">\(\bm{Q}^T=\bm{Q}^{-1}\)</span> and <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon})  &amp;= \hat{\bm{e}}^T
\hat{\bm{e}}  + \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix}
\right]^T \bm{R}^T \bm{R} \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix}
\right] \\
&amp;= \hat{\bm{e}}^T \hat{\bm{e}} + \left|\left| \bm{R} \left[
\begin{matrix} \bm{\upsilon} - \hat{\bm{\upsilon}} \\ \bm{\beta} -
\hat{\bm{\beta}} \end{matrix} \right] \right| \right|^2  \label{eq:r2F}
\end{align}\]</span> Notice that <span class="math inline">\(\hat{\bm{e}}^T \hat{\bm{e}}\)</span> is the value
of <span class="math inline">\(r^2\)</span> evaluated at the least
squares solution (denoted by adding hats to <span class="math inline">\(\bm{\beta}\)</span> and <span class="math inline">\(\bm{\upsilon}\)</span>), so that <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}}) &amp;=
\hat{\bm{e}}^T \hat{\bm{e}} \label{eq:r2tau}
\end{align}\]</span> Plugging eqs. <span class="math inline">\(\ref{eq:r2tau}\)</span> and <span class="math inline">\(\ref{eq:Rblock}\)</span> into eq. <span class="math inline">\(\ref{eq:r2F}\)</span> (Bates et al., 2015, eq.
19), <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon})  &amp;= r^2(\bm{\theta},
\bm{\hat{\beta}}, \bm{\hat{\upsilon}}) + \left|\left| \left[
\begin{matrix} \bm{R}_{11} &amp; \bm{R}_{12} \\ \bm{0} &amp;
\bm{R}_{22}  \end{matrix}\right] \left[ \begin{matrix} \bm{\upsilon} -
\hat{\bm{\upsilon}} \\ \bm{\beta} - \hat{\bm{\beta}} \end{matrix}
\right] \right| \right|^2 \\
&amp;= r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}}) +
\left|\left|  \bm{R}_{11} (\bm{\upsilon} - \hat{\bm{\upsilon}}) +
\bm{R}_{12}  (\bm{\beta} - \hat{\bm{\beta}})  \right| \right|^2  +
\left|\left|  \bm{R}_{22}  (\bm{\beta} - \hat{\bm{\beta}})  \right|
\right|^2 \label{eq:r2sub}
\end{align}\]</span></p>
<p>From the joint distribution of <span class="math inline">\(\bm{y}\)</span> and <span class="math inline">\(\bm{\upsilon}\)</span> (Bates et al., 2015, eqs.
20, 21, and 22), <span class="math display">\[\begin{align}
(\bm{y},\bm{\upsilon}) &amp;\sim N(\bm{y} - \bm{X\beta} -
\bm{Z\upsilon},\sigma^2 \bm{I}_{n_x}) * N(\bm{\upsilon},  \sigma^2
\bm{I}_{n_z})
\end{align}\]</span> and the probability density function of the joint
distribution of <span class="math inline">\(\bm{y}\)</span> and <span class="math inline">\(\bm{\upsilon}\)</span> is <span class="math display">\[\begin{align}
f_{\bm{y}, \bm{\upsilon}} \left(\bm{y}, \bm{\upsilon}, \bm{\beta},
\bm{\theta}, \sigma^2 \right)&amp;=
\frac{1}{\left(2\pi\sigma^2\right)^{\frac{n_x}{2}}}
\exp{\left[\frac{-r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) +
||\upsilon||^2}{2\sigma^2} \right]}  \cdot
\frac{1}{\left(2\pi\sigma^2\right)^{\frac{n_z}{2}}} \exp{\left[
\frac{-||\bm{\upsilon}||^2}{2\sigma^2}\right] }  \\
&amp;= \frac{1}{\left(2\pi\sigma^2\right)^{\frac{n_x+n_z}{2}}}
\exp{\left[\frac{-r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon})
}{2\sigma^2}\right] }
\end{align}\]</span> This can then be integrated over <span class="math inline">\(\bm{\upsilon}\)</span> to get the likelihood of
the model (Bates et al., 2015, eqs. 25 and 26): <span class="math display">\[\begin{align}
\mathcal{L}\left( \bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}
\right)&amp;=\int f_{\bm{y}, \bm{\upsilon}} \left(\bm{y}, \bm{\upsilon},
\bm{\beta}, \bm{\theta}\right) d\bm{\upsilon}\\
&amp;=\int \frac{1}{\left(2\pi\sigma^2\right)^{\frac{n_x+n_z}{2}}}
\exp{\frac{-r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) }{2\sigma^2}
}  d\bm{\upsilon}\\
&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{n_x+n_z}{2}}}
\exp{\frac{-r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}}) -
\left|\left|  \bm{R}_{22}  (\bm{\beta} - \hat{\bm{\beta}})  \right|
\right|^2 }{2\sigma^2} } \int \exp{\frac{- \left|\left|  \bm{R}_{11}
(\bm{\upsilon} - \hat{\bm{\upsilon}}) + \bm{R}_{12}  (\bm{\beta} -
\hat{\bm{\beta}})  \right| \right|^2 }{2\sigma^2} }  d\bm{\upsilon}
\label{eq:lnlInt}
\end{align}\]</span> This can be solved with a change of variables
(Bates et al., 2015, eq. 27): <span class="math display">\[\begin{align}
\bm{\gamma} &amp;=  \bm{R}_{11} (\bm{\upsilon} - \hat{\bm{\upsilon}}) +
\bm{R}_{12}  (\bm{\beta} - \hat{\bm{\beta}}) \label{eq:gamma} \\
\frac{d  \bm{\gamma}}{d \bm{\upsilon}}&amp;= \bm{R}_{11}
\label{eq:Jgamma}
\end{align}\]</span> Using the change of variables formula, we add the
inverse determinant of <span class="math inline">\(\bm{R}_{11}\)</span>
when plugging eq. <span class="math inline">\(\ref{eq:gamma}\)</span>
into <span class="math inline">\(\ref{eq:lnlInt}\)</span>, and using eq.
<span class="math inline">\(\ref{eq:Jgamma}\)</span> (Bates et al.,
2015, eq. 28): <span class="math display">\[\begin{align}
\mathcal{L}\left( \bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}
\right)&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{n_x+n_z}{2}}}
\exp{\frac{-r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}}) -
\left|\left|  \bm{R}_{22}  (\bm{\beta} - \hat{\bm{\beta}})  \right|
\right|^2 }{2\sigma^2} } \int \exp{\frac{- \left|\left|  \gamma  \right|
\right|^2 }{2\sigma^2} } | {\rm det} (\bm{R}_{11}) |^{-1} d\bm{\gamma}
\\
&amp;=\frac{1}{|{\rm det}(\bm{R}_{11})| \left(2\pi\sigma^2\right)^{
\frac{n_x}{2}}} \exp{\frac{-r^2(\bm{\theta}, \bm{\hat{\beta}},
\bm{\hat{\upsilon}}) - \left|\left|  \bm{R}_{22}  (\bm{\beta} -
\hat{\bm{\beta}})  \right| \right|^2 }{2\sigma^2} }  \left\{
\frac{1}{\left(2\pi\sigma^2\right)^{ \frac{n_z}{2}} } \int \exp{\frac{-
\left|\left|  \gamma  \right| \right|^2 }{2\sigma^2} }  d\bm{\gamma}
\right\}
\end{align}\]</span> and because the term in curly braces is now of a
probability density function, it integrates to 1: <span class="math display">\[\begin{align}
\mathcal{L} \left( \bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}
\right)&amp;=\frac{1}{|{\rm det}(\bm{R}_{11})|
\left(2\pi\sigma^2\right)^{\frac{n_x}{2}}} \exp{\frac{-r^2(\bm{\theta},
\bm{\hat{\beta}}, \bm{\hat{\upsilon}}) -
\left|\left|  \bm{R}_{22}  (\bm{\beta} - \hat{\bm{\beta}})  \right|
\right|^2 }{2\sigma^2} }  \label{eq:WeMixL0}
\end{align}\]</span> Having solved the integral symbolically, this
expression for the likelihood no longer has an explicit integral and has
been calculated exactly, without use of numerical quadrature. This
derivation is incredibly close to Bates et al. (2015), with the only
modification being that we use the QR decomposition of <span class="math inline">\(\bm{A}\)</span> where they used the Cholesky
decomposition of <span class="math inline">\(\bm{A}^T
\bm{A}\)</span>.</p>
<p>This makes a deviance function, defined relative to the
log-likelihood (<span class="math inline">\(\ell\)</span>), so that
<span class="math inline">\(D(\cdot) = -2\ell(\cdot)\)</span>, <span class="math display">\[\begin{align}
D \left( \bm{\beta}, \bm{\theta}, \sigma^2; \bm{y} \right)&amp;= 2\,{\rm
ln} {|{\rm det}(\bm{R}_{11})|} + n_x {\rm ln} \left(2\pi\sigma^2\right)
+ \frac{r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}}) +
\left|\left|  \bm{R}_{22}  (\bm{\beta} - \hat{\bm{\beta}})  \right|
\right|^2 }{\sigma^2}  \label{eq:WeMixD0}
\end{align}\]</span></p>
<p>Using the profile likelihood, the deviance is minimized when <span class="math inline">\(\bm{\beta}=\hat{\bm{\beta}}\)</span> because <span class="math inline">\(\bm{\beta}\)</span> only appears inside of a sum
of squares that can be minimized (set to zero) using <span class="math inline">\(\bm{\beta}=\hat{\bm{\beta}}\)</span> (Bates et
al., 2015, eqs. 30 and 31). The profile deviance then becomes <span class="math display">\[\begin{align}
D \left( \bm{\theta}, \sigma^2; \bm{y} \right)&amp;= 2\,{\rm ln} {|{\rm
det}(\bm{R}_{11})|} + n_x {\rm ln} \left(2\pi\sigma^2\right) +
\frac{r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}})
}{\sigma^2}  
\end{align}\]</span> Similarly, the value of <span class="math inline">\(\sigma^2\)</span> can be found by taking the
derivative of the profile deviance with respect to <span class="math inline">\(\sigma^2\)</span> and setting it equal to zero.
This yields <span class="math display">\[\begin{align}
\widehat{\sigma^2}&amp;= \frac{r^2(\bm{\theta}, \bm{\hat{\beta}},
\bm{\hat{\upsilon}})}{n_x}  
\end{align}\]</span> giving a profile deviance that is a function only
of the parameter <span class="math inline">\(\bm{\theta}\)</span>: <span class="math display">\[\begin{align}
D \left( \bm{\theta}; \bm{y} \right)&amp;= 2\,{\rm ln} {|{\rm
det}(\bm{R}_{11})|} + n_x \left({\rm ln} \left(2\pi
\widehat{\sigma^2}\right) + 1 \right)
\end{align}\]</span></p>
<p>The purpose of <code>WeMix</code> is to estimate the likelihood of
the weighted model (eqs. <span class="math inline">\(\ref{eq:WeMixA}\)</span> and <span class="math inline">\(\ref{eq:WeMixB}\)</span>). In this section we
derive the weighted estimator that is analogous to the estimator used by
<code>lme4</code> and is similar to Henderson (1982) but we include a
deviance (and likelihood), which Henderson omits.</p>
<p>The first difference is in the penalized sum of squares, which now
weights the residuals by the unit-level weights (<span class="math inline">\(\bm{\Omega}\)</span>) and the random-effect
penalties by the group-level weights (<span class="math inline">\(\bm{\Psi}\)</span>): <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left|
\bm{\Omega}^{\frac{1}{2}}\left( \bm{y}-\bm{X\beta} - \sum_{l=1}^L
\bm{Z}_l\Lambda_{ll}(\theta)\upsilon_l \right)\right|\right|^2 +
\sum_{l=2}^{L} \left|\left| \left(\bm{\Psi}_{ll}\right)^{\frac{1}{2}}
\bm{\upsilon}_l\right|\right|^2 \label{eq:r2sep}
\end{align}\]</span> where <span class="math inline">\(\bm{\Omega}\)</span> and <span class="math inline">\(\bm{\Psi}_{ll}\)</span> are diagonal matrices with
unconditional inverse probability of selection for each unit (<span class="math inline">\(\bm{\Omega}\)</span>) or group (<span class="math inline">\(\bm{\Psi}_{ll}\)</span>) along its diagonal. The
unconditional probability that a unit or group was selected can be
readily calculated as the product of a probability of its own
probability of selection and the unconditional probability of the group
to which it belongs.</p>
<p>Then, the weighted pseudo-data notation combines the two terms in eq.
<span class="math inline">\(\ref{eq:r2sep}\)</span>, adding a vector of
<em>pseudo-data</em> to the end of the <span class="math inline">\(\bm{y}\)</span> vector: <span class="math display">\[\begin{align}
r^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left| \left[
\begin{matrix} \bm{\Omega}^{\frac{1}{2}} \bm{y} \\ \bm{0} \end{matrix}
\right] - \left[ \begin{matrix} \bm{\Omega}^{\frac{1}{2}}
\bm{Z\Lambda}(\bm{\theta}) &amp; \bm{\Omega}^{\frac{1}{2}}\bm{X} \\
\bm{\Psi}^{\frac{1}{2}} &amp; \bm{0} \end{matrix} \right] \left[
\begin{matrix} \bm{\upsilon} \\ \bm{\beta} \end{matrix} \right]
\right|\right|^2 \label{eq:WeMixWr2} \\
&amp;= \left|\left| \bm{\Omega}^{\frac{1}{2}} \left( \bm{y} -
\bm{Z\Lambda}(\bm{\theta}) \bm{\upsilon} - \bm{X} \right) \right|
\right|^2 + \left| \left| \bm{\Psi}^{\frac{1}{2}} \bm{\upsilon} \right|
\right|^2
\end{align}\]</span> where <span class="math inline">\(\bm{Z}\)</span>
is now a block matrix that incorporates all of the <span class="math inline">\(\bm{Z}\)</span> matrices for the various levels:
<span class="math display">\[\begin{align}
\bm{Z} = \left[ \begin{matrix} \bm{Z}_1 &amp; \bm{Z}_2 &amp; \cdots
&amp; \bm{Z}_L \end{matrix} \right]
\end{align}\]</span> <span class="math inline">\(\bm{\Lambda}(\bm{\theta})\)</span> is a block
diagonal matrix with elements <span class="math inline">\(\bm{\Lambda}_{ll}(\bm{\theta})\)</span>, <span class="math inline">\(\bm{\Psi}\)</span> is a block diagonal matrix with
elements <span class="math inline">\(\bm{\Psi}_{ll}\)</span>, and <span class="math display">\[\begin{align}
\bm{\upsilon} = \left[ \begin{matrix} \bm{\upsilon}_1 \\ \bm{\upsilon}_2
\\ \vdots  \\ \bm{\upsilon}_L  \end{matrix} \right]
\end{align}\]</span></p>
<p>The likelihood of <span class="math inline">\(\bm{y}\)</span>,
conditional on <span class="math inline">\(\bm{\upsilon}\)</span> is
then <span class="math display">\[\begin{align}
f_{\bm{y}|\bm{\upsilon}=\bm{\upsilon}}(\bm{y}, \bm{\upsilon}) &amp;=
\prod_{i=1}^{n_x} \left[
{\frac{1}{\left(2\pi\sigma^2\right)^{\frac{1}{2}}}
\exp\left[-\frac{\left|\left|\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{Z}_i\bm{\Lambda}(\bm{\theta})\bm{\upsilon}
\right|\right|^2}{2\sigma^2}\right) } \right]^{\bm{\Omega}_{ii}} \\
&amp;=
\prod_{i=1}^{n_x}  {\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\bm{\Omega}_{ii}}{2}}}
\exp \left(- \frac{ \left|\left| \bm{\Omega}_{ii}^{\frac{1}{2}}
\left(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{Z}_i\bm{\Lambda}(\bm{\theta})\bm{\upsilon}
\right) \right|\right|^2}{2\sigma^2}\right) } \\
&amp;= {\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i
\bm{\Omega}_{ii}}{2}}} \exp \left( - \frac{
\left|\left|\bm{\Omega}^{\frac{1}{2}}
\left(\bm{y}-\bm{X}\bm{\beta}-\bm{Z}\bm{\Lambda}(\bm{\theta})\bm{\upsilon}
\right) \right|\right|^2}{2\sigma^2}\right) } \label{eq:WeMixWfyu}
\end{align}\]</span> And the unconditional density of <span class="math inline">\(\bm{\upsilon}\)</span> is <span class="math display">\[\begin{align}
f_{\bm{\upsilon}}(\bm{\upsilon}) &amp;=\prod_{j=1}^{n_z} \left[
\frac{1}{\left(2\pi\sigma^2\right)^{\frac{1}{2}}} \exp \left[- \frac{
\left|\left|  \bm{\upsilon}_j
\right|\right|^2}{2\sigma^2}\right]  \right]^{\bm{\Psi}_{jj}} \\
&amp;=\prod_{j=1}^{n_z}
\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\bm{\Psi}_{jj}}{2}}} \exp
\left[- \frac{ \left|\left|
\bm{\Psi}_{jj}^{\frac{1}{2}}  \bm{\upsilon}_j
\right|\right|^2}{2\sigma^2}\right] \\
&amp;= \frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_j
\bm{\Psi}_{jj}}{2}}} \exp \left[- \frac{
\left|\left|  \bm{\Psi}^{\frac{1}{2}}  \bm{\upsilon}
\right|\right|^2}{2\sigma^2}\right] \label{eq:WeMixWfu}
\end{align}\]</span> where <span class="math inline">\(\sum
\bm{\Psi}_{jj}\)</span> is the sum of the terms in the diagonal matrix
<span class="math inline">\(\bm{\Psi}\)</span>.</p>
<p>The joint distribution of <span class="math inline">\(\bm{\upsilon}\)</span> and <span class="math inline">\(\bm{y}\)</span> is then the product of eqs. <span class="math inline">\(\ref{eq:WeMixWfyu}\)</span> and <span class="math inline">\(\ref{eq:WeMixWfu}\)</span>: <span class="math display">\[\begin{align}
f_{\bm{y}, \bm{\upsilon}} \left(\bm{y}, \bm{\upsilon}, \bm{\beta},
\bm{\theta}, \sigma^2 \right)&amp;=
f_{\bm{y}|\bm{\upsilon}=\bm{\upsilon}}(\bm{y}, \bm{\upsilon})\cdot
f_{\bm{\upsilon}}(\bm{\upsilon})\\
&amp;={\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i
\bm{\Omega}_{ii}}{2}}} \exp \left[- \frac{
\left|\left|\bm{\Omega}^{\frac{1}{2}}
\left(\bm{y}-\bm{X\beta}-\bm{Z}\bm{\Lambda}(\bm{\theta})\bm{\upsilon}  \right)
\right|\right|^2}{2\sigma^2}\right] } \cdot
\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_j \bm{\Psi}_{jj}}{2}}}
\exp \left[- \frac{ \left|\left|  \bm{\Psi}^{\frac{1}{2}}  \bm{\upsilon}
\right|\right|^2}{2\sigma^2}\right] \\
&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i \bm{\Omega}_{ii}
+ \sum_j \bm{\Psi}_{jj}}{2}}} \exp \left[- \frac{
\left|\left|\bm{\Omega}^{\frac{1}{2}}
\left(\bm{y}-\bm{X\beta}-\bm{Z}\bm{\Lambda}(\bm{\theta})\bm{\upsilon}
\right) \right|\right|^2 +
\left|\left|  \bm{\Psi}^{\frac{1}{2}}  \bm{\upsilon}
\right|\right|^2}{2\sigma^2} \right] \\
&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i \bm{\Omega}_{ii}
+ \sum_j \bm{\Psi}_{jj}}{2}}} \exp \left[- \frac{ r^2(\bm{\theta},
\bm{\beta}, \bm{\upsilon})}{2\sigma^2} \right]
\end{align}\]</span> Using the same logic for the results in eq. <span class="math inline">\(\ref{eq:r2sub}\)</span>, <span class="math inline">\(r^2\)</span> can be written as a sum of the value
at the optimum (<span class="math inline">\(\hat{\bm{\beta}}\)</span>
and <span class="math inline">\(\hat{\bm{\upsilon}}\)</span>) and
deviations from that: <span class="math display">\[\begin{align}
f_{\bm{y}, \bm{\upsilon}} \left(\bm{y}, \bm{\upsilon}, \bm{\beta},
\bm{\theta}, \sigma^2
\right)&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i
\bm{\Omega}_{ii} + \sum_j \bm{\Psi}_{jj}}{2}}} \exp \left[- \frac{
r^2(\bm{\theta}, \hat{\bm{\beta}}, \hat{\bm{\upsilon}}) - \left| \left|
\bm{R}_{22}(\bm{\beta} - \hat{\bm{\beta}})\right| \right|^2 - \left|
\left| \bm{R}_{11}(\bm{\upsilon} - \hat{\bm{\upsilon}}) +
\bm{R}_{12}(\bm{\beta} - \hat{\bm{\beta}}) \right| \right|^2}{2\sigma^2}
\right]
\end{align}\]</span> Now, finding the integral of this over <span class="math inline">\(\bm{\upsilon}\)</span>, <span class="math display">\[\begin{align}
\mathcal{L}(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y})&amp;=\int
f_{\bm{y}, \bm{\upsilon}} \left(\bm{y}, \bm{\upsilon}, \bm{\beta},
\bm{\theta}, \sigma^2 \right)  d\bm{\upsilon} \\
&amp;=\int \frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i
\bm{\Omega}_{ii} + \sum_j \bm{\Psi}_{jj}}{2}}} \exp \left[- \frac{
r^2(\bm{\theta}, \hat{\bm{\beta}}, \hat{\bm{\upsilon}}) - \left| \left|
\bm{R}_{22}(\bm{\beta} - \hat{\bm{\beta}})\right| \right|^2 - \left|
\left| \bm{R}_{11}(\bm{\upsilon} - \hat{\bm{\upsilon}}) +
\bm{R}_{12}(\bm{\beta} - \hat{\bm{\beta}}) \right| \right|^2}{2\sigma^2}
\right] d\bm{\upsilon} \\
&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i \bm{\Omega}_{ii}
+ \sum_j \bm{\Psi}_{jj}}{2}}} \exp \left[- \frac{ r^2(\bm{\theta},
\hat{\bm{\beta}}, \hat{\bm{\upsilon}}) - \left| \left|
\bm{R}_{22}(\bm{\beta} - \hat{\bm{\beta}})\right| \right|^2
}{2\sigma^2}\right] \int \exp \left[- \frac{\left|
\left|  \bm{R}_{11}(\bm{\upsilon} - \hat{\bm{\upsilon}}) +
\bm{R}_{12}(\bm{\beta} - \hat{\bm{\beta}}) \right| \right|^2}{2\sigma^2}
\right] d\bm{\upsilon} \label{eq:tmpLA}
\end{align}\]</span> Notice that while the unweighted integral has <span class="math inline">\(n_z\)</span> dimensions, this weighted integral
has <span class="math inline">\(\sum_j \bm{\Psi}_{jj}\)</span>
dimensions—the number of (population) individuals values to integrate
out. <span class="math display">\[\begin{align}
\mathcal{L}(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y})
&amp;=\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i
\bm{\Omega}_{ii}}{2}}} \exp \left[- \frac{ r^2(\bm{\theta},
\hat{\bm{\beta}}, \hat{\bm{\upsilon}}) - \left| \left|
\bm{R}_{22}(\bm{\beta} - \hat{\bm{\beta}})\right| \right|^2
}{2\sigma^2}\right] \\ &amp; \left\{
\frac{1}{(2\pi\sigma^2)^{\frac{\sum_j \bm{\Psi}_{jj}}{2}}} \int \exp
\left[- \frac{\left| \left|  \bm{R}_{11}(\bm{\upsilon} -
\hat{\bm{\upsilon}}) + \bm{R}_{12}(\bm{\beta} - \hat{\bm{\beta}})
\right| \right|^2}{2\sigma^2} \right] d\bm{\upsilon} \right\}
\label{eq:tmpLB}
\end{align}\]</span> However, while we know there are <span class="math inline">\(\sum_j \bm{\Psi}_{jj}\)</span> dimensions to
integrate out (the number of population cases), a change of variables
must maintain the dimensionality of the integration, so it is not clear
how to proceed. Instead, we name the term inside the integral <span class="math inline">\(\alpha\)</span> and use a different methodology to
derive its value. Then, <span class="math display">\[\begin{align}
\mathcal{L}(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y})
&amp;= \alpha(\bm{\theta};\bm{\Omega},\bm{\Psi})
\frac{1}{\left(2\pi\sigma^2\right)^{\frac{\sum_i \bm{\Omega}_{ii}}{2}}}
\exp \left[- \frac{ r^2(\bm{\theta}, \hat{\bm{\beta}},
\hat{\bm{\upsilon}}) - \left| \left| \bm{R}_{22}(\bm{\beta} -
\hat{\bm{\beta}})\right| \right|^2 }{2\sigma^2}\right] \label{eq:tmpL2}
\end{align}\]</span> where <span class="math inline">\(\alpha\)</span>
is a constant for a fixed <span class="math inline">\(\bm{\theta}\)</span> and set of weights <span class="math inline">\(\bm{\Omega}\)</span> and <span class="math inline">\(\bm{\Psi}\)</span>.</p>
<p>While these formulas allow for estimation of a likelihood function
that allows for estimation of <span class="math inline">\(\hat{\bm{\beta}}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> via profiling, they do not
depend on <span class="math inline">\(\alpha\)</span> because <span class="math inline">\(\bm{\theta}\)</span> appears as a parameter of
<span class="math inline">\(\alpha\)</span>; optimizing the
log-likelihood with respect to <span class="math inline">\(\bm{\theta}\)</span> requires all of the terms in
the log-likelihood to be calculated, including <span class="math inline">\(\alpha\)</span>.</p>
<p>Bates and Pinheiro (1998) offer an unweighted method of calculating
<span class="math inline">\(\alpha\)</span> that we extend here to admit
weighting. The essential insight of Bates and Pinheiro is that the
variables must be remapped, per group, using an orthogonal transform
that separates out the <span class="math inline">\(q_l\)</span> random
effects associated with group <span class="math inline">\(g\)</span> at
level <span class="math inline">\(l\)</span>. In what follows we
describe a three-level case, but the methods readily generalize to the
<span class="math inline">\(L\)</span>-level case.</p>
<p>The integral is slightly re-expressed using <span class="math inline">\(\bm{u}\)</span> instead of <span class="math inline">\(\upsilon\)</span>, but instead of using <span class="math inline">\(\bm{\Lambda}\)</span> it uses the <span class="math inline">\(\bm{\Delta}\)</span> matrix, which is an
individual block of <span class="math inline">\(\bm{\Lambda}\)</span>,
so <span class="math inline">\(\bm{\Delta}\)</span> is defined,
implicitly, by <span class="math display">\[\begin{align}
\bm{\Lambda}(\bm{\theta}) = \bm{\Delta}(\bm{\theta})  \otimes \bm{I}
\end{align}\]</span> where <span class="math inline">\(\otimes\)</span>
is the Kronecker product.</p>
<p>Using this notation, the likelihood is then given by <span class="math display">\[\begin{align}
\mathcal{L}&amp;=\int \prod_g \left[ \int f_{y|U} (\bm{y}, \bm{\theta},
\bm{u}_{2g}, \cdots, \bm{\upsilon}_{Lg} ) f_U (\bm{\theta}, \bm{u}_{2g})
d\bm{u}_{2g}  \right] f_{U} (\bm{\theta}, \bm{u}_{3g})  d\bm{u}_{3g}
\label{eq:intalpha} \\
&amp;\propto \int \prod_g \left[ \int \exp \left[ \left | \left
|\Omega_{gg}^{\frac{1}{2}} (\bm{y}_g - \bm{X}_g \bm{\beta} - \bm{Z}_g
\bm{u})  \right | \right|^2 + \left| \left|\bm{u}^T_{2g} \bm{\Delta}^T
\bm{\Delta} \bm{u}_{2g} \right|\right|^2 \right] d\bm{u}_{2g} \right]
f_{U} (\bm{\theta}, \bm{u}_{3g})  d\bm{u}_{3g} \label{eq:intalpha2}
\end{align}\]</span> and iteratively rewritten to symbolically integrate
out the lowest level random effects, starting with level 2 and
increasing until there are no remaining integrals. When this is done, we
will note the change of variable associated with a weighted model and
use that for <span class="math inline">\(\alpha\)</span> in eq. <span class="math inline">\(\ref{eq:tmpL2}\)</span>. Because of that, the
portions unrelated to the change of variable were dropped from relation
<span class="math inline">\(\ref{eq:intalpha2}\)</span>. Thus, notice
that this goal is just for units in a particular group (<span class="math inline">\(g\)</span>) at level 2. These results of several
integrals have to be combined to solve the integral across all groups
and calculate <span class="math inline">\(\alpha\)</span>. The value of
<span class="math inline">\(\alpha\)</span> will be calculated by level
and then summed; for a three-level model: <span class="math display">\[\begin{align}
\alpha = \alpha_2 + \alpha_3
\end{align}\]</span></p>
<p>While the residual sum of squares (<span class="math inline">\(r^2\)</span>) has been, up until now, treated for
the entire data set, the notion is to think of the residual sum of
squares for just group <span class="math inline">\(g\)</span> (at level
<span class="math inline">\(l\)</span>). Bates and Pinheiro (1998) also
use the notion of <span class="math inline">\(r^2_g\)</span>, or the
<span class="math inline">\(r^2\)</span> contribution for group <span class="math inline">\(g\)</span>, which is defined as <span class="math display">\[\begin{align}
r_g^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left|
\left[ \begin{matrix} \bm{\Omega}^{\frac{1}{2}}_{gg}\bm{y}_g \\ \bm{0}
\end{matrix} \right] - \left[ \begin{matrix}
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{g}  &amp;
\bm{\Omega}^{\frac{1}{2}}_{gg} X_g \\ \bm{\Delta}_{l}^\prime &amp;
\bm{0} \end{matrix} \right] \left[ \begin{matrix} \bm{u}_{g} \\
\bm{\beta} \end{matrix} \right]  \right| \right|^2
\end{align}\]</span> where <span class="math inline">\(\bm{y}_g\)</span>, <span class="math inline">\(\bm{\upsilon}_g\)</span>, <span class="math inline">\(\bm{X}_g\)</span>, <span class="math inline">\(\bm{Z}_g\)</span>, and <span class="math inline">\(\bm{\Omega}_{gg}\)</span> are the rows of the
<span class="math inline">\(\bm{y}\)</span> vector, the <span class="math inline">\(\bm{\upsilon}\)</span> vector, <span class="math inline">\(\bm{X}\)</span> matrix, <span class="math inline">\(\bm{Z}\)</span> matrix, and <span class="math inline">\(\bm{\Omega}\)</span> matrix that are associated
with group <span class="math inline">\(g\)</span>, respectively; while
<span class="math inline">\(\bm{\Lambda}(\bm{\theta})\)</span> is the
full <span class="math inline">\(\bm{\Lambda}(\theta)\)</span> matrix,
<span class="math inline">\(\bm{\Delta}_l^\prime\)</span> is a block
matrix: <span class="math display">\[\begin{align}
\bm{\Delta}_{l}^\prime &amp; \equiv \left[ \begin{matrix}\bm{\Delta}_{l}
&amp; \bm{0} \end{matrix} \right]
\end{align}\]</span> where <span class="math inline">\(\bm{\Delta}_l\)</span> is the portion of <span class="math inline">\(\bm{\Delta}\)</span> associated with level <span class="math inline">\(l\)</span>, and the <span class="math inline">\(\bm{0}\)</span> matrix is entirely zeros and
conforms to the portion of <span class="math inline">\(\bm{u}_{g}\)</span> that is associated with level
3 of the model.</p>
<p>Expanding <span class="math inline">\(\bm{Z}_{g}\)</span> gives <span class="math display">\[\begin{align}
r_g^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left|
\left[ \begin{matrix} \bm{\Omega}^{\frac{1}{2}}_{gg}\bm{y}_g \\ \bm{0}
\end{matrix} \right] - \left[ \begin{matrix}
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{2g} &amp;
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{3g} &amp;
\bm{\Omega}^{\frac{1}{2}}_{gg} X_g \\ \bm{\Delta}_2 &amp; \bm{0}&amp;
\bm{0} \end{matrix} \right] \left[ \begin{matrix} \bm{\upsilon}_{2g} \\
\bm{\upsilon}_{3g} \\ \bm{\beta} \end{matrix} \right]  \right| \right|^2
\label{eq:hugerg}
\end{align}\]</span></p>
<p>Starting at level 2, a change of variables is chosen via the (full)
QR decomposition, <span class="math display">\[\begin{align}
\left[ \begin{matrix} \bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{2 g}  \\
\bm{\Delta}_2 \end{matrix} \right] &amp;= \bm{Q}_{2g} \left[
\begin{matrix} \bm{R}_{12g}  \\ \bm{0} \end{matrix} \right]
\label{eq:QR0}
\end{align}\]</span> where the subscript on <span class="math inline">\(\bm{R}_{12g}\)</span> indicates that it is the top
submatrix (<span class="math inline">\(1\)</span>), at level 2, and for
group <span class="math inline">\(g\)</span>. Notice that the blocks are
different shapes on the left- and right-hand sides of eq. <span class="math inline">\(\ref{eq:QR0}\)</span>; on the left-hand side, the
top block (<span class="math inline">\(\bm{\Omega}^{\frac{1}{2}}_{gg}
\bm{Z}_{2 g}\)</span>) has as many rows as there are observations in
group <span class="math inline">\(g\)</span> and the bottom block (<span class="math inline">\(\bm{\Delta}_2\)</span>) has as many rows as there
are random effects at level 2, while the right-hand side is flipped. The
top block (<span class="math inline">\(\bm{R}_{12g}\)</span>) has as
many rows as there are random effects at level 2, while the bottom block
(<span class="math inline">\(\bm{0}\)</span>) has as many rows as there
are observations in group <span class="math inline">\(g\)</span>. The
reason for this is that the change makes the top block on the right-hand
side a square, upper triangular matrix, which will allow the change of
variables to proceed.</p>
<p>Because <span class="math inline">\(\bm{Q}_{2g}\)</span> is
orthogonal by construction, <span class="math inline">\(\bm{Q}_{2g}^T
\bm{Q}_{2g}= I\)</span>, one can freely premultiply the inside of the
sum of squares in eq. <span class="math inline">\(\ref{eq:hugerg}\)</span> by <span class="math inline">\(\bm{Q}_{2g}^T\)</span> without changing the sum of
squares: <span class="math display">\[\begin{align}
r_g^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon}) &amp;= \left| \left|
\bm{Q}_{2g}^T \left\{ \left[ \begin{matrix}
\bm{\Omega}^{\frac{1}{2}}_{gg}\bm{y}_g \\ \bm{0} \end{matrix} \right] -
\left[ \begin{matrix} \bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{2g}  &amp;
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{3g}  &amp;
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{X}_g \\ \bm{\Delta}_2 &amp; \bm{0}
&amp; \bm{0} \end{matrix} \right] \left[ \begin{matrix}
\bm{\upsilon}_{2g} \\ \bm{\upsilon}_{3g} \\ \bm{\beta} \end{matrix}
\right] \right\}  \right| \right|^2 \label{eq:Qz}
\end{align}\]</span> Then, defining <span class="math display">\[\begin{align}
\bm{Q}_{2g}^T \left[ \begin{matrix}
\bm{\Omega}^{\frac{1}{2}}_{gg}\bm{y}_g \\ \bm{0} \end{matrix} \right]
&amp; \equiv  \left[ \begin{matrix} \bm{R}_{1yg} \\ \bm{R}_{2yg}
\end{matrix} \right] &amp; \bm{Q}_{2g}^T \left[ \begin{matrix}
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{Z}_{3g}  \\ \bm{0} \end{matrix}
\right] &amp; \equiv  \left[ \begin{matrix} \bm{R}_{13g} \\ \bm{R}_{23g}
\end{matrix} \right] &amp; \bm{Q}_{2g}^T \left[ \begin{matrix}
\bm{\Omega}^{\frac{1}{2}}_{gg} \bm{X}_{g}  \\ \bm{0} \end{matrix}
\right] &amp; \equiv  \left[ \begin{matrix} \bm{R}_{1Xg} \\ \bm{R}_{2Xg}
\end{matrix} \right] \label{eq:Qtdef}
\end{align}\]</span> similar to eq. <span class="math inline">\(\ref{eq:QR0}\)</span>, and with the same
dimensions, the blocks are of different shapes on the left and right of
the equation.</p>
<p>Multiplying the <span class="math inline">\(\bm{Q}_{2g}^T\)</span>
through and plugging the eq. <span class="math inline">\(\ref{eq:Qtdef}\)</span> equations into eq. <span class="math inline">\(\ref{eq:Qz}\)</span>, <span class="math display">\[\begin{align}
r_g^2(\bm{\theta}, \bm{\beta}, \bm{u})  &amp;= \left| \left| \left[
\begin{matrix} \bm{R}_{1yg} \\ \bm{R}_{2yg} \end{matrix} \right]
-  \left[ \begin{matrix} \bm{R}_{12g} &amp; \bm{R}_{13g}
&amp;  \bm{R}_{1Xg} \\ \bm{0} &amp; \bm{R}_{23g} &amp; \bm{R}_{2Xg}
\end{matrix} \right] \left[ \begin{matrix} \bm{u}_{2g} \\ \bm{u}_{3g} \\
\bm{\beta} \end{matrix} \right]  \right| \right|^2
\end{align}\]</span> it is now possible to simplify the integral, do a
change of variables and integrate out level 2 random effects for group
<span class="math inline">\(g\)</span>, and solve the first integral in
eq. <span class="math inline">\(\ref{eq:intalpha2}\)</span>
symbolically. In particular, this rewriting of the terms means there are
<span class="math inline">\(q_2\)</span> terms with <span class="math inline">\(\bm{u}_{2g}\)</span> in them and <span class="math inline">\(n_g - q_2\)</span> terms that are redefined to be
orthogonal to those two terms. It is this orthogonality that allows the
other terms to be removed from the integral. So, since we have just
shown <span class="math display">\[\begin{align}
||\Omega_{gg}^{\frac{1}{2}} (\bm{y}_g - \bm{X}_g \bm{\beta} - \bm{Z}_g
\bm{\Lambda}_{ll} (\bm{\theta}) \bm{u})  ||^2 + ||\bm{u}||^2 =&amp;
\left| \left| \left[ \begin{matrix} \bm{R}_{1yg} \\ \bm{R}_{2yg}
\end{matrix} \right] -  \left[ \begin{matrix} \bm{R}_{12g} &amp;
\bm{R}_{13g} &amp; \bm{R}_{1Xg} \\ \bm{0} &amp; \bm{R}_{23g} &amp;
\bm{R}_{2Xg} \end{matrix} \right] \left[ \begin{matrix} \bm{u}_{2g} \\
\bm{u}_{3g} \\ \bm{\beta} \end{matrix} \right]  \right| \right|^2 \\
\begin{split}
=&amp; \left| \left| \bm{R}_{1yg} -   \bm{R}_{12g} \bm{u}_{2g} -
\bm{R}_{13g} \bm{u}_{3g}  - \bm{R}_{1Xg} \bm{\beta}_{g} \right|
\right|^2  \\
&amp;+ \left| \left| \bm{R}_{2yg}  - \bm{R}_{23g}\bm{u}_{3g} -
\bm{R}_{2Xg} \bm{\beta}_{g}  \right| \right|^2
\end{split}
\end{align}\]</span> which can now be substituted into eq. <span class="math inline">\(\ref{eq:intalpha2}\)</span>, allowing a change of
variables: <span class="math display">\[\begin{align}
\bm{\gamma}_{2g} =&amp;  \bm{R}_{1yg} -   \bm{R}_{12g} \bm{u}_{2g} -
\bm{R}_{13g} \bm{u}_{3g} - \bm{R}_{1Xg} \bm{\beta}_{g} \\
\frac{d\bm{\gamma}_{2g}}{d\bm{u}_{2g}} =&amp; \bm{R}_{12g}
\end{align}\]</span> The value of <span class="math inline">\(\alpha_2\)</span> in the unweighted case is now
clear: <span class="math display">\[\begin{align}
\alpha_{2u} =&amp; \sum_{g=1}^{n_2} |{\rm det}(\bm{R}_{12g})|^{-1}
\end{align}\]</span> where <span class="math inline">\(\alpha_{2u}\)</span> is the unweighted alpha. This
formula can be weighted simply by applying the replicate weights to the
individual terms: <span class="math display">\[\begin{align}
\alpha_{2} =&amp; \sum_{g=1}^{n_2} \bm{\Psi}_{gg} |{\rm
det}(\bm{R}_{12g})|^{-1}
\end{align}\]</span> However, for the three-level model, the likelihood
still has level 3 integrals. The level 3 integral can also be removed.
We cannot restart this process with the original <span class="math inline">\(\bm{Z}\)</span> and <span class="math inline">\(\bm{X}\)</span> matrices and other components
because they change with the components inside the level 2 integral.
However, the <span class="math inline">\(\bm{R}_{2**}\)</span> matrices
are the portions of the higher level <span class="math inline">\(\bm{u}_{3g}\)</span> that were not integrated out,
and they can be used independent of <span class="math inline">\(\bm{u}_{2g}\)</span>.</p>
<p>We continue on with these remapped variables, starting the unweighted
case (only at level 2), and now using <span class="math inline">\(g'\)</span> to indicate that this is a
different group (at level 3), with <span class="math inline">\(n_{g'}\)</span> subgroups in it. Each group
(labeled <span class="math inline">\(i\)</span>) contributes an outcomes
matrix <span class="math inline">\(\bm{R}_{2yi}\)</span>, a matrix per
level 2 group <span class="math inline">\(\bm{R}_{23i}\)</span> and a
matrix per fixed effects regressor <span class="math inline">\(\bm{X}_{2Xi}\)</span>, for <span class="math inline">\(i=1, \cdots, n_{g'}\)</span>. Combining these,
the residual sum of squares at level 3 for the group <span class="math inline">\(g'\)</span> is <span class="math display">\[\begin{align}
r_{g'}^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon})  &amp;= \left|
\left| \left[ \begin{matrix} \bm{R}_{2y1} \\ \vdots \\
\bm{R}_{2yn_{g'}} \\ \bm{0} \end{matrix} \right] -  \left[
\begin{matrix}   \bm{R}_{231} &amp; \bm{R}_{2X1} \\ \vdots &amp; \vdots
\\  \bm{R}_{23n_{g'}} &amp; \bm{R}_{2Xn_{g'}}  \\
\bm{\Delta}_{3} &amp; \bm{0} \end{matrix} \right] \left[ \begin{matrix}
\bm{u}_{3g} \\ \bm{\beta} \end{matrix} \right]  \right| \right|^2
\label{eq:RunW}
\end{align}\]</span> Following the example at level 2 above, the QR
decomposition is then used: <span class="math display">\[\begin{align}
\left[ \begin{matrix} \bm{R}_{231} \\ \vdots \\ \bm{R}_{23n_{g'}} \\
\bm{\Delta}_{3} \end{matrix} \right] = \bm{Q}_{3g',u} \left[
\begin{matrix} \bm{R}_{13g',u}  \\ \bm{0} \end{matrix} \right]
\end{align}\]</span> where a subscript <span class="math inline">\(u\)</span> is used to indicate that <span class="math inline">\(\bm{Q}_{3g',u}\)</span> and <span class="math inline">\(\bm{R}_{13g',u}\)</span> are unweighted. The
remaining steps are then identical to the level 2 case; <span class="math inline">\(\bm{R}_{13g',u}\)</span> is used as the change
of variables, so that <span class="math inline">\(\alpha_{3u} =
\sum_{g'=1}^{n_2} |{\rm det}(\bm{R}_{13g',u})|^{-1}\)</span>,
and the other <span class="math inline">\(\bm{R}_{3**}\)</span> matrices
can be used to integrate out level-4 cases and so on.</p>
<p>When there are level 2 conditional weights—non-unit probabilities of
selection for the groups at level 2, conditional on the selection of the
level 3 unit—each matrix in eq. <span class="math inline">\(\ref{eq:RunW}\)</span> could be replicated <span class="math inline">\(\bm{\Psi}_{ii}\)</span> times. Equivalently, each
matrix can be weighted by the conditional probability of selection, so
that eq. <span class="math inline">\(\ref{eq:RunW}\)</span> becomes
<span class="math display">\[\begin{align}
r_{g'}^2(\bm{\theta}, \bm{\beta}, \bm{\upsilon})  &amp;= \left|
\left| \left[ \begin{matrix} \bm{\Psi}_{11}^{\frac{1}{2}} \bm{R}_{2y1}
\\ \vdots \\ \bm{\Psi}_{g'g'}^{\frac{1}{2}}
\bm{R}_{2yn_{g'}} \\ \bm{0} \end{matrix} \right] -  \left[
\begin{matrix}   \bm{\Psi}_{11}^{\frac{1}{2}} \bm{R}_{231}
&amp;  \bm{\Psi}_{11}^{\frac{1}{2}} \bm{R}_{2X1} \\ \vdots &amp; \vdots
\\  \bm{\Psi}_{g'g'}^{\frac{1}{2}} \bm{R}_{23n_{g'}} &amp;
\bm{\Psi}_{g'g'}^{\frac{1}{2}} \bm{R}_{2Xn_{g'}}  \\
\bm{\Delta}_{3} &amp; \bm{0} \end{matrix} \right] \left[ \begin{matrix}
\bm{u}_{3g} \\ \bm{\beta} \end{matrix} \right]  \right| \right|^2
\label{eq:R}
\end{align}\]</span></p>
<p>This change leads to the same QR decomposition as the replicated
case: <span class="math display">\[\begin{align}
\left[  \begin{matrix} \bm{\Psi}_{11}^{\frac{1}{2}} \bm{R}_{231} \\
\vdots \\ \bm{\Psi}_{g'g'}^{\frac{1}{2}} \bm{R}_{23n_{g'}}
\\ \bm{\Delta}_{3} \end{matrix} \right] = \bm{Q}_{3g'} \left[
\begin{matrix} \bm{R}_{13g'}  \\ \bm{0} \end{matrix} \right]
\label{eq:QRg3}
\end{align}\]</span></p>
<p>The weighted value of <span class="math inline">\(\alpha_3\)</span>
for the third level is then <span class="math display">\[\begin{align}
\alpha_{3} = \sum_{g'=1}^{n_3} \bm{\Psi}_{g'g'} \left|
\bm{R}_{13g'} \right|^{-1}
\end{align}\]</span></p>
<p>The actual implementation of the calculation is slightly different
from what is above. First, when the QR decomposition is taken (eqs.
<span class="math inline">\(\ref{eq:QR0}\)</span> and <span class="math inline">\(\ref{eq:QRg3}\)</span>), it is possible to stop
the decomposition as is described in Bates and Pinheiro (1998). It is
also possible to continue the QR decomposition for the other levels of
<span class="math inline">\(\bm{Z}\)</span>. Using the <span class="math inline">\(\bm{QR}\)</span> on the entire matrix still
results in an orthogonal component in the submatrices, and so meets the
goals of the decomposition while obviating the need to form the <span class="math inline">\(\bm{Q}\)</span> matrix explicitly.</p>
<p>Also note that, in the above, the value of <span class="math inline">\(r^2\)</span> was never used, so the components
relating to <span class="math inline">\(\bm{y}\)</span> and <span class="math inline">\(\bm{X}\)</span> need not be formed.</p>
<p>Continuing to follow <code>lme4</code>, the estimation uses the
profile likelihood. Since <span class="math inline">\(\bm{\beta}\)</span> appears only in the final term
in quadratic form, it is immediately evident that the maximum likelihood
estimator (MLE) for <span class="math inline">\(\bm{\beta}\)</span> is
<span class="math inline">\(\hat{\bm{\beta}}\)</span>, making eq. <span class="math inline">\(\ref{eq:tmpL2}\)</span> profile to <span class="math display">\[\begin{align}
D \left( \bm{\theta}, \sigma^2; \bm{y} \right)&amp;= 2\,\rm{ln}
\left(\alpha (\bm{\theta};\bm{\Psi},\bm{\Omega}) \right) + \left(\sum_i
\bm{\Omega}_{ii} \right) \rm{ln} \left(2\pi\sigma^2\right) +
\frac{r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}})}{\sigma^2}
\label{eq:WeMixPB}
\end{align}\]</span></p>
<p>Then, the value of <span class="math inline">\(\sigma^2\)</span> can
also be profiled out by taking the derivative of the deviance with
respect to <span class="math inline">\(\sigma^2\)</span> and setting it
equal to zero (Bates et al., 2015, eq. 32): <span class="math display">\[\begin{align}
0 &amp;= \frac{\sum_i \bm{\Omega}_{ii}}{\widehat{\sigma^2}} -
\frac{r^2(\bm{\theta}, \bm{\hat{\beta}}, \bm{\hat{\upsilon}})
}{\widehat{\sigma^4}}
\end{align}\]</span> rearranging <span class="math display">\[\begin{align}
\widehat{\sigma^2} &amp;=  \frac{r^2(\bm{\theta}, \bm{\hat{\beta}},
\bm{\hat{\upsilon}}) }{\sum_i \bm{\Omega}_{ii}}  \label{eq:WeMixMaxs2}
\end{align}\]</span> Eq. <span class="math inline">\(\ref{eq:WeMixMaxs2}\)</span> can then be plugged
into eq. <span class="math inline">\(\ref{eq:WeMixPB}\)</span> to give
<span class="math display">\[\begin{align}
D \left( \bm{\theta}; \bm{y} \right)&amp;= 2\, \rm{ln} \left(\alpha
(\bm{\theta};\bm{\Psi},\bm{\Omega}) \right)  + \left(\sum_i
\bm{\Omega}_{ii}\right) \left[ \rm{ln}
\left(2\pi\widehat{\sigma^2}\right) + 1 \right] \label{eq:WeMixP}
\end{align}\]</span> This function is then minimized numerically with
respect to <span class="math inline">\(\bm{\theta}\)</span>, using the
profile estimates for <span class="math inline">\(\bm{\beta}\)</span>
and <span class="math inline">\(\bm{\upsilon}\)</span> (eq. <span class="math inline">\(\ref{eq:WeMixWr2}\)</span>) and <span class="math inline">\(\widehat{\sigma^2}\)</span> (eq. <span class="math inline">\(\ref{eq:WeMixP}\)</span>).</p>
<p>The estimated values are then the <span class="math inline">\(\bm{\theta}\)</span> that maximize eq. <span class="math inline">\(\ref{eq:WeMixP}\)</span>, the <span class="math inline">\(\sigma^2\)</span> value from eq. <span class="math inline">\(\ref{eq:WeMixMaxs2}\)</span>, and the <span class="math inline">\(\bm{\beta}\)</span> values from solving the system
of equations in eq. <span class="math inline">\(\ref{eq:WeMixWr2}\)</span>.</p>
</div>
<div class="section level2">
<h2 id="variance-estimation">Variance Estimation<a class="anchor" aria-label="anchor" href="#variance-estimation"></a>
</h2>
<p>The inverse Hessian of <span class="math inline">\(\bm{\beta}\)</span> is given by (Bates et al.,
2015, eq. 54): <span class="math display">\[\begin{align}
\widehat{\rm{Var}}(\bm{\beta}) &amp;= \widehat{\sigma^2}
\bm{R}_{22}^{-1} \left(\bm{R}^T_{22}\right)^{-1}
\end{align}\]</span> with <span class="math inline">\(\bm{R}_{22}\)</span> coming from eq. <span class="math inline">\(\ref{eq:tmpL2}\)</span>. This variance estimator
assumes that the weights are information weights and so is inappropriate
for survey weights.</p>
<p>A robust (sandwich) variance estimator is given by (Binder, 1983) is
appropriate: <span class="math display">\[\begin{align}
\left(\widehat{\sigma^2} \bm{R}^T_{22}\right)^{-1} \bm{J}
\left(\widehat{\sigma^2} \bm{R}^T_{22}\right)^{-1} \label{eq:sandwich}
\end{align}\]</span> where <span class="math inline">\(\bm{J}\)</span>
is the sum of outer products of the Jacobian matrix <span class="math display">\[\begin{align}
\bm{J} = \frac{n_L}{n_L-1} \sum_{g=1}^{n_L}
\frac{\partial(\ell_g)}{\partial \bm{\beta}}
\end{align}\]</span> where <span class="math inline">\(n_L\)</span> is
the number of level-<span class="math inline">\(L\)</span> (top-level)
groups, <span class="math inline">\(g\)</span> indexes level-<span class="math inline">\(L\)</span> groups, and <span class="math inline">\(\ell_g\)</span> is the log-likelihood for group
<span class="math inline">\(g\)</span> and all groups and units nested
inside of <span class="math inline">\(g\)</span>. The log-likelihood of
the full model is <span class="math display">\[\begin{align}
\ell(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}) &amp;= \rm{ln} \left[
\alpha(\bm{\theta};\bm{\Omega},\bm{\Psi}) \right] - \frac{\sum_{i}
\bm{\Omega}_{ii}}{2} \rm{ln} \left(2\pi\sigma^2\right) - \frac{
r^2\left(\bm{\theta}, \hat{\bm{\beta}}, \hat{\bm{\upsilon}}\right)}{2
\sigma^2} - \frac{\left| \left| \bm{R}_{22}\left(\hat{\bm{\beta}} -
\bm{\beta}\right)\right| \right|^2 }{2\sigma^2} \label{eq:Vlnl}
\end{align}\]</span> where we are allowing <span class="math inline">\(\bm{\beta}\)</span> and <span class="math inline">\(\bm{\theta}\)</span> to vary but are fixing <span class="math inline">\(\sigma^2\)</span> at the estimated value of <span class="math inline">\(\hat{\sigma}^2\)</span>. This could have been
annotated by making <span class="math inline">\(\hat{\bm{\beta}}(\bm{\theta})\)</span> because
<span class="math inline">\(\hat{\bm{\beta}}\)</span> is the estimated
value conditional on <span class="math inline">\(\bm{\theta}\)</span>
and appears in the equation separate from the value of <span class="math inline">\(\bm{\beta}\)</span>, but that is not shown
here.</p>
<p>While it would be convenient if eq. <span class="math inline">\(\ref{eq:Vlnl}\)</span> could be directly broken up
into a portion attributable to each group, and some encouragement
appears when the first three terms can be, the final term has
dependencies across multiple groups. A distinct likelihood is needed
that depends only on the data in that group. This is achieved by noting
that data for a particular group is also valid data for a mixed model of
the same type as the global mixed model, and so eq. <span class="math inline">\(\ref{eq:Vlnl}\)</span> can be used on a single
group’s data to get the group log-likelihood; thus a group
log-likelihood can be written using the notion of the fitted value of
<span class="math inline">\(\bm{\beta}\)</span> in the group (<span class="math inline">\(\hat{\bm{\beta}}_g\)</span>) <span class="math display">\[\begin{align}
\ell_g(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}_g) &amp;= \rm{ln}
\left[ \alpha_g(\bm{\theta};\bm{\Omega},\bm{\Psi}) \right] -
\frac{\sum_{i\in g} \bm{\Omega}_{ii}}{2} \rm{ln}
\left(2\pi\sigma^2\right) - \frac{ r^2\left(\bm{\theta},
\hat{\bm{\beta}}_g, \hat{\bm{\upsilon}}_g\right)}{2\sigma^2} -
\frac{\left| \left| \bm{R}_{22g}\left(\hat{\bm{\beta}}_g -
\bm{\beta}\right)\right| \right|^2 }{2\sigma^2} \label{eq:Vlnlg}
\end{align}\]</span> where <span class="math inline">\(\alpha_g\)</span>
is the <span class="math inline">\(\alpha\)</span> term for group <span class="math inline">\(g\)</span> and any groups nested in it, the sum
for <span class="math inline">\(\Omega\)</span> is just over <span class="math inline">\(i\)</span> terms associated with group <span class="math inline">\(g\)</span>, <span class="math inline">\(\hat{\bm{\beta}}\)</span> and <span class="math inline">\(\hat{\bm{\upsilon}}\)</span> are the values fitted
only on this group, and <span class="math inline">\(\bm{R}_{22g}\)</span> is the result of a QR on a
version of <span class="math inline">\(\bm{A}\)</span> performed on just
data (<span class="math inline">\(\bm{X}\)</span>, <span class="math inline">\(\bm{Z}\)</span>, <span class="math inline">\(\bm{y}\)</span>, <span class="math inline">\(\bm{\Psi}\)</span>, and <span class="math inline">\(\bm{\Omega}\)</span>) associated with group <span class="math inline">\(g\)</span>, while the values of <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\bm{\beta}\)</span>, and <span class="math inline">\(\bm{\theta}\)</span> are the values from the value
the function is being evaluated at globally. Then, <span class="math display">\[\begin{align}
\ell(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}) = \sum_g
\ell_g(\bm{\beta}, \bm{\theta}, \sigma^2; \bm{y}_g)
\end{align}\]</span> A few notes are required at this point on how,
exactly, this is calculated in degenerate cases. When the matrix <span class="math inline">\(\bm{A}_g\)</span> is singular for a group (e.g.,
when there is only one unit in the group), then the inestimable values
of <span class="math inline">\(\bm{\beta}_g\)</span> are set to zero
when forming <span class="math inline">\(\hat{\bm{\beta}}_g -
\bm{\beta}\)</span>. Similarly, <span class="math inline">\(\bm{R}_{22g}\)</span> may not have enough rows to
form a upper-triangular matrix. The portion that can be formed
(including all columns) is then used—this does not affect the
computation of <span class="math inline">\(\bm{R}_{22g}\left(\hat{\bm{\beta}}_g -
\bm{\beta}\right)\)</span>.</p>
<p>Using these formulas the Jacobian matrix can now be calculated
numerically and the robust variance estimator formed with eq. <span class="math inline">\(\ref{eq:sandwich}\)</span>.</p>
<p>So far this section has regarded only <span class="math inline">\(\bm{\beta}\)</span> but similar methods apply to
the estimation of the variance of the random effect variance estimates
(<span class="math inline">\(\bm{\theta}\)</span> and <span class="math inline">\(\sigma\)</span>). These variance terms have their
variance estimated assuming that they are uncorrelated with the <span class="math inline">\(\bm{\beta}\)</span> terms. At each level the
variance is calculated, including a term for <span class="math inline">\(\sigma\)</span>, as <span class="math display">\[\begin{align}
{\rm Var} \left(\bm{\theta},\sigma \right) = \left( -\bm{H} \right)^{-1}
\bm{J}_{\bm{\theta},\sigma} \left( -\bm{H} \right)^{-1} \label{vartheta}
\end{align}\]</span> where <span class="math inline">\(\bm{H}\)</span>
is the Hessian of the likelihood (eq. <span class="math inline">\(\ref{eq:Vlnl}\)</span>) with respect to <span class="math inline">\(\bm{\theta}\)</span> and <span class="math inline">\(\sigma\)</span> while <span class="math inline">\(\bm{J}_{\bm{\theta},\sigma}\)</span> is the
portion of the Jacobian that regards <span class="math inline">\(\bm{\theta}\)</span> and <span class="math inline">\(\sigma\)</span>. The estimated value for the
variance of <span class="math inline">\(\sigma\)</span> from the lowest
level group (level 2) is used to form the standard error of the residual
variance.</p>
<p>However, the variance estimates are not simply the values of <span class="math inline">\(\bm{\theta}\)</span> and <span class="math inline">\(\sigma\)</span> but transformations of that (eq.
<span class="math inline">\(\ref{eq:root}\)</span>). To estimate the
variances of the variance estimates, the Delta method is used so that
<span class="math display">\[\begin{align}
{\rm Var}\left(\bm{\Sigma}, \sigma^2 \right) = \left[ \nabla (
\bm{\Lambda}^T \bm{\Lambda} ) \right]^T {\rm Var}\left(\bm{\theta},
\sigma^2 \right) \left[ \nabla  ( \bm{\Lambda}^T \bm{\Lambda}) \right]
\end{align}\]</span> where the gradient (<span class="math inline">\(\nabla (\cdot)\)</span>) is taken with respect to
the elements of <span class="math inline">\(\bm{\Sigma}\)</span> and
<span class="math inline">\(\sigma^2\)</span>, and <span class="math inline">\(\rm{Var}\left(\bm{\theta}, \sigma^2
\right)\)</span> is from eq. <span class="math inline">\(\ref{vartheta}\)</span>.</p>
<div class="section level3">
<h3 id="model-evaluation-wald-test">Model Evaluation: Wald Test<a class="anchor" aria-label="anchor" href="#model-evaluation-wald-test"></a>
</h3>
<p>We can use the a Wald test to test both fixed effects parameters
(<span class="math inline">\(\beta\)</span>) and variance of the random
parameters (<span class="math inline">\(\Lambda\)</span>).</p>
<p>The Wald test compares estimated parameters with null hypothesis
values. In the default case the null hypothesis is that value of the
parameters is 0.</p>
<p>In this default case, if the test fails to reject the null
hypothesis, removing the variables from the model will not substantially
harm the fit of that model.</p>
<p>One advantage of the Wald test is that it can be used to test
multiple hypotheses about multiple parameters simultaneously.</p>
<p>To test <span class="math inline">\(q\)</span> hypotheses on <span class="math inline">\(p\)</span> estimated parameters, let <span class="math inline">\(\hat{P}\)</span> be the vector of estimated
coefficients, <span class="math inline">\(R\)</span> be a <span class="math inline">\(q\)</span> x <span class="math inline">\(p\)</span> hypothesis matrix (this matrix has 1
row per coefficient being tested with a value of 1 in the column
corresponding to that coefficient), <span class="math inline">\(\hat{V}\)</span> be the estimated covariance
matrix for <span class="math inline">\(\hat{P}\)</span>, and <span class="math inline">\(r\)</span> be the vector of hypothesized values
for <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>Then the Wald test statistic for multiple parameters is equal to:</p>
<p><span class="math display">\[
W =(R\hat{\beta} - r)'(R\hat{V}R')^{-1}(R\hat{\beta} - r)
\]</span></p>
<p>The resulting test statistic can be tested against a chi-square
distribution. For this test, the degrees of freedom is the number of
parameters that are tested.</p>
<p><span class="math display">\[ W \sim \chi^2(p) \]</span></p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Bates, D., Machler, M., Bolker, B. M., &amp; Walker, S. C. (2015),
Fitting linear mixed-effects models using lme4. <em>Journal of
Statistical Software</em>, <em>67</em>(<em>1</em>), 1–48.</p>
<p>Bates, D., &amp; Pinheiro, J. C. (1998). <em>Computational methods
for multilevel modelling.</em> Bell Labs Report.</p>
<p>Binder, D. A. (1983). On the variances of asymptotically normal
estimators from complex surveys. <em>International Statistical
Review</em>, <em>51</em>(<em>3</em>), 279–292.</p>
<p>Gelman, A. (2007). Struggles with survey weighting and regression
modeling. <em>Statistical Science</em>, <em>22</em>(<em>2</em>),
153–164.</p>
<p>Henderson, C. R. (1982). Analysis of covariance in the mixed model:
higher-level, nonhomogeneous, and random regressions.
<em>Biometrics</em>, <em>38</em>(<em>3</em>), 623–640.</p>
<p>Integration by Substitution. (n.d.). In Wikipedia. Retrieved February
13, 2019, from </p>
<p>Rabe-Hesketh, S., &amp; Skrondal, A. (2006). Multilevel modelling of
complex survey data. <em>Journal of the Royal Statistical Society</em>.
Series A (Statistics in Society), <em>169</em>(<em>4</em>), 805–827.</p>
<p>Rabe-Hesketh, S., Skrondal, A., &amp; Pickles, A. (2002). Reliable
estimation of generalized linear mixed models using adaptive quadrature.
<em>Stata Journal</em>, <em>2</em>, 1–21.</p>
<p>Trefethen, L. N., &amp; Bau, D. (1997). <em>Numerical linear
algebra</em>. Philadelphia, PA: SIAM.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Paul Bailey, Blue Webb, Claire Kelley, Trang Nguyen, Huade Huo.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
